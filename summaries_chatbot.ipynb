{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000223724CA6E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000223724F80A0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcript.txt\",\"r\") as file:\n",
    "    transcript_content=file.read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document: Internal Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Analytics', 'summary': 'Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data, which also falls under and directly relates to the umbrella term, data science. Analytics also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.\\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to International Data Corporation, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.', 'source': 'https://en.wikipedia.org/wiki/Analytics'}, page_content='Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data, which also falls under and directly relates to the umbrella term, data science. Analytics also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.\\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to International Data Corporation, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.  \\n\\n\\n== Analytics vs analysis ==\\nData analysis focuses on the process of examining past data through business understanding, data understanding, data preparation, modeling and evaluation, and deployment. It is a subset of data analytics, which takes multiple data analysis processes to focus on why an event happened and what may happen in the future based on the previous data. Data analytics is used to formulate larger organizational decisions. \\nData analytics is a multidisciplinary field. There is extensive use of computer skills, mathematics, statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data through analytics. There is increasing use of the term advanced analytics, typically used to describe the technical aspects of analytics, especially in the emerging fields such as the use of machine learning techniques like neural networks, decision trees, logistic regression, linear to multiple regression analysis, and classification to do predictive modeling. It also includes unsupervised machine learning techniques like cluster analysis, Principal Component Analysis, segmentation profile analysis and association analysis.\\n\\n\\n== Applications ==\\n\\n\\n=== Marketing optimization ===\\nMarketing organizations use analytics to determine the outcomes of campaigns or efforts, and to guide decisions for investment and consumer targeting. Demographic studies, customer segmentation, conjoint analysis and other techniques allow marketers to use large amounts of consumer purchase, survey and panel data to understand and communicate marketing strategy.\\nMarketing analytics consists of both qualitative and quantitative, structured and unstructured data used to drive strategic decisions about brand and revenue outcomes. The process involves predictive modelling, marketing experimentation, automation and real-time sales communications. The data enables companies to make predictions and alter strategic execution to maximize performance results.\\nWeb analytics allows marketers to collect session-level information about interactions on a website using an operation called sessionization. Google Analytics is an example of a popular free analytics tool that marketers use for this purpose. Those interactions provide web analytics information systems with the information necessary to track the referrer, search keywords, identify the IP address, and track the activities of the visitor. With this information, a marketer can improve marketing campaigns, website creative content, and information architecture.\\nAnalysis techniques frequently used in marketing inc'), Document(metadata={'title': 'Big data', 'summary': 'Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.\\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17Ã—260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than â‚¬100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"', 'source': 'https://en.wikipedia.org/wiki/Big_data'}, page_content='Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.\\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17Ã—260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than â‚¬100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, h'), Document(metadata={'title': 'Predictive analytics', 'summary': 'Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.\\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Predictive_analytics'}, page_content='Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.\\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\\n\\n\\n== Definition ==\\nPredictive analytics is a set of business intelligence (BI) technologies that uncovers relationships and patterns within large volumes of data that can be used to predict behavior and events. Unlike other BI technologies, predictive analytics is forward-looking, using past events to anticipate the future. Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs. The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.\\nPredictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analyticsâ€”Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\" In future industrial systems, the value of predictive analytics will be to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization.\\n\\n\\n== Analytical techniques ==\\nThe approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.\\n\\n\\n=== Machine Learning ===\\n\\nMachine learning can be defined as the ability of a machine to learn and then mimic human behavior that requires intelligence. This is accomplished through artificial intelligence, algorithms, and models.\\n\\n\\n=== Autoregressive Integrated Moving Average (ARIMA) ===\\n\\nARIMA models are a common example of time series models. These models use autoregression, which means the model can be fitted with a regression software that will use machine learning to do most of the regression analysis and smoothing. ARIMA models are known to have no overall trend, but instead have a variation around the average that has a constant amplitude, resulting in statistically similar time patterns. Through this, variables are analyzed and data is filtered in order to better understand and p'), Document(metadata={'title': 'Protiviti', 'summary': \"Protiviti Inc. (Protiviti) is a global consulting firm headquartered in Menlo Park and San Ramon, California, that provides consulting in internal audit, risk and compliance, technology, business processes, data analytics and finance. It is a subsidiary under Robert Half. Protiviti and its independently and locally owned Member Firms serve clients through a network of more than 85 locations in over 27 countries.\\nProtiviti has served more than 80 percent of Fortune 100, nearly 80 percent of Fortune 500 and 70 percent of Fortune 1000 companies. From 2018 to 2022, the firm has been consistently listed by Forbes\\u2063 as being one of the world's best management consulting firms. Protiviti has also been listed as one of the 100 Best Companies to Work For by Fortune Magazine for 8 consecutive years from 2015 to 2022.\", 'source': 'https://en.wikipedia.org/wiki/Protiviti'}, page_content='Protiviti Inc. (Protiviti) is a global consulting firm headquartered in Menlo Park and San Ramon, California, that provides consulting in internal audit, risk and compliance, technology, business processes, data analytics and finance. It is a subsidiary under Robert Half. Protiviti and its independently and locally owned Member Firms serve clients through a network of more than 85 locations in over 27 countries.\\nProtiviti has served more than 80 percent of Fortune 100, nearly 80 percent of Fortune 500 and 70 percent of Fortune 1000 companies. From 2018 to 2022, the firm has been consistently listed by Forbes\\u2063 as being one of the world\\'s best management consulting firms. Protiviti has also been listed as one of the 100 Best Companies to Work For by Fortune Magazine for 8 consecutive years from 2015 to 2022.\\n\\n\\n== History ==\\nProtiviti was formed in 2002 when the Company hired more than 700 professionals who had been affiliated with the internal audit, business and technology risk consulting practice of Arthur Andersen, including more than 50 individuals who had been partners of that firm. These professionals formed the base of Protiviti.\\nIn 2006, Protiviti acquired the assets of PG Lewis & Associates, a leading national provider of Data Forensics and Cybersecurity services founded in 2003 by serial technology entrepreneur, Paul G. Lewis. Financial terms were not disclosed.\\nThe following year Protiviti acquired the bankruptcy consulting firm PENTA Advisory Services, LLC with locations in Baltimore, Maryland and Richmond, Virginia. PENTA provided restructuring and insolvency services, litigation services and US bankruptcy trustee services. Financial details regarding the transaction were not disclosed.\\nIn January 2019, Protiviti expanded its Middle East & North African presence by launching an office in Cairo, Egypt. The new location is the first Member Firm in North Africa, and is led by Managing Director Ashraf Fahmy, a former Deloitte partner in Egypt and with the firm\\'s enterprise risk practice in Abu Dhabi.\\nIn February 2019, Protiviti added Gauteng-based internal audit and forensic services firm SekelaXabiso CA (SkX) as its first member firm in South Africa. The new firm will serve domestic firms as well as international firms looking for support to enter the South African market. The firm houses over 200 consultants, and has offices in the major financial centres of Gauteng and Durban.\\nFor fiscal year 2019, Protiviti\\'s revenue exceeded US$1 billion for the first time in its 18-year history.\\nIn March 2020, Protiviti expanded its European footprint by opening 3 new offices in ZÃ¼rich, Switzerland, and Berlin and DÃ¼sseldorf, Germany. Protiviti\\'s global network of member firms operate as independently owned and operated entities, but have access to the firm\\'s resources despite not possessing agency to act on Protiviti\\'s behalf.\\n\\n\\n== Service Expansion ==\\nIn March 2022, Protiviti formed a new service line, \"Protiviti Digital\", which serves as a digital marketing agency. Protiviti Digital serves clients seeking to execute complex digital and marketing strategies, as well as transform customer experiences.\\nIn April 2022, Robert Half moved its legal consulting service line to Protiviti, which allows Protiviti to expand its legal consulting practice. The service line supports clients with a broader range of legal, compliance, governance, technology, investigation and transaction-related business needs.\\n\\n\\n== See also ==\\nList of management consulting firms\\nList of IT consulting firms\\nRobert Half\\nArthur Andersen\\n\\n\\n== References =='), Document(metadata={'title': 'Fractal Analytics', 'summary': 'Fractal Analytics Private Limited, trading as Fractal, is a multinational artificial intelligence company which provides services in packaged consumer goods, insurance, healthcare, life sciences, retail, technology, and the financial sector. The company has dual headquarters in Mumbai and New York City, with a presence in the United States, India, and the United Kingdom, among other locations.', 'source': 'https://en.wikipedia.org/wiki/Fractal_Analytics'}, page_content='Fractal Analytics Private Limited, trading as Fractal, is a multinational artificial intelligence company which provides services in packaged consumer goods, insurance, healthcare, life sciences, retail, technology, and the financial sector. The company has dual headquarters in Mumbai and New York City, with a presence in the United States, India, and the United Kingdom, among other locations.\\n\\n\\n== History ==\\nFractal Analytics was founded in 2000 in Mumbai by Srikanth Velamakanni, Pranay Agrawal, Nirmal Palaparthi, Pradeep Suryanarayan and Ramakrishna Reddy. It later moved to the US in 2005. In 2015 they acquired Imagna Analytics and Mobius Innovations.\\nIn 2016, Fractal Analytics appointed Pranay Agrawal as the CEO to replace co-founder Srikanth Velamakanni, the new Group Chief Executive and Executive Vice-Chairman. It also expanded its operations including the creation of two new subsidiaries Qure.ai and Cuddle.ai.\\nIn August 2016, they partnered with KNIME, an open source data analytics platform. In June 2017, they acquired Chicago-based strategy & analytics firm, 4i Inc. In September 2017, they partnered with Final Mile to combine data science with behavioral science  In March 2018, Fractal Analytics acquired behavioural architecture company Final Mile. In January 2019, Fractal received a $200 million funding from Apax Partners.\\nIn January 2022, Fractal became a unicorn company after raising $360 million from private equity firm TPG.\\n\\n\\n== References ==')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"Data Analytics in Finance\", load_max_docs=5).load()\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Analytics', 'summary': 'Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data, which also falls under and directly relates to the umbrella term, data science. Analytics also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.\\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to International Data Corporation, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.', 'source': 'https://en.wikipedia.org/wiki/Analytics'}, page_content='Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data, which also falls under and directly relates to the umbrella term, data science. Analytics also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.\\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to International Data Corporation, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.  \\n\\n\\n== Analytics vs analysis ==\\nData analysis focuses on the process of examining past data through business understanding, data understanding, data preparation, modeling and evaluation, and deployment. It is a subset of data analytics, which takes multiple data analysis processes to focus on why an event happened and what may happen in the future based on the previous data. Data analytics is used to formulate larger organizational decisions. \\nData analytics is a multidisciplinary field. There is extensive use of computer skills, mathematics, statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data through analytics. There is increasing use of the term advanced analytics, typically used to describe the technical aspects of analytics, especially in the emerging fields such as the use of machine learning techniques like neural networks, decision trees, logistic regression, linear to multiple regression analysis, and classification to do predictive modeling. It also includes unsupervised machine learning techniques like cluster analysis, Principal Component Analysis, segmentation profile analysis and association analysis.\\n\\n\\n== Applications ==\\n\\n\\n=== Marketing optimization ===\\nMarketing organizations use analytics to determine the outcomes of campaigns or efforts, and to guide decisions for investment and consumer targeting. Demographic studies, customer segmentation, conjoint analysis and other techniques allow marketers to use large amounts of consumer purchase, survey and panel data to understand and communicate marketing strategy.\\nMarketing analytics consists of both qualitative and quantitative, structured and unstructured data used to drive strategic decisions about brand and revenue outcomes. The process involves predictive modelling, marketing experimentation, automation and real-time sales communications. The data enables companies to make predictions and alter strategic execution to maximize performance results.\\nWeb analytics allows marketers to collect session-level information about interactions on a website using an operation called sessionization. Google Analytics is an example of a popular free analytics tool that marketers use for this purpose. Those interactions provide web analytics information systems with the information necessary to track the referrer, search keywords, identify the IP address, and track the activities of the visitor. With this information, a marketer can improve marketing campaigns, website creative content, and information architecture.\\nAnalysis techniques frequently used in marketing inc'),\n",
       " Document(metadata={'title': 'Big data', 'summary': 'Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.\\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17Ã—260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than â‚¬100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"', 'source': 'https://en.wikipedia.org/wiki/Big_data'}, page_content='Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.\\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17Ã—260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than â‚¬100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, h'),\n",
       " Document(metadata={'title': 'Predictive analytics', 'summary': 'Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.\\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Predictive_analytics'}, page_content='Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.\\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\\n\\n\\n== Definition ==\\nPredictive analytics is a set of business intelligence (BI) technologies that uncovers relationships and patterns within large volumes of data that can be used to predict behavior and events. Unlike other BI technologies, predictive analytics is forward-looking, using past events to anticipate the future. Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs. The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.\\nPredictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analyticsâ€”Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\" In future industrial systems, the value of predictive analytics will be to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization.\\n\\n\\n== Analytical techniques ==\\nThe approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.\\n\\n\\n=== Machine Learning ===\\n\\nMachine learning can be defined as the ability of a machine to learn and then mimic human behavior that requires intelligence. This is accomplished through artificial intelligence, algorithms, and models.\\n\\n\\n=== Autoregressive Integrated Moving Average (ARIMA) ===\\n\\nARIMA models are a common example of time series models. These models use autoregression, which means the model can be fitted with a regression software that will use machine learning to do most of the regression analysis and smoothing. ARIMA models are known to have no overall trend, but instead have a variation around the average that has a constant amplitude, resulting in statistically similar time patterns. Through this, variables are analyzed and data is filtered in order to better understand and p'),\n",
       " Document(metadata={'title': 'Protiviti', 'summary': \"Protiviti Inc. (Protiviti) is a global consulting firm headquartered in Menlo Park and San Ramon, California, that provides consulting in internal audit, risk and compliance, technology, business processes, data analytics and finance. It is a subsidiary under Robert Half. Protiviti and its independently and locally owned Member Firms serve clients through a network of more than 85 locations in over 27 countries.\\nProtiviti has served more than 80 percent of Fortune 100, nearly 80 percent of Fortune 500 and 70 percent of Fortune 1000 companies. From 2018 to 2022, the firm has been consistently listed by Forbes\\u2063 as being one of the world's best management consulting firms. Protiviti has also been listed as one of the 100 Best Companies to Work For by Fortune Magazine for 8 consecutive years from 2015 to 2022.\", 'source': 'https://en.wikipedia.org/wiki/Protiviti'}, page_content='Protiviti Inc. (Protiviti) is a global consulting firm headquartered in Menlo Park and San Ramon, California, that provides consulting in internal audit, risk and compliance, technology, business processes, data analytics and finance. It is a subsidiary under Robert Half. Protiviti and its independently and locally owned Member Firms serve clients through a network of more than 85 locations in over 27 countries.\\nProtiviti has served more than 80 percent of Fortune 100, nearly 80 percent of Fortune 500 and 70 percent of Fortune 1000 companies. From 2018 to 2022, the firm has been consistently listed by Forbes\\u2063 as being one of the world\\'s best management consulting firms. Protiviti has also been listed as one of the 100 Best Companies to Work For by Fortune Magazine for 8 consecutive years from 2015 to 2022.\\n\\n\\n== History ==\\nProtiviti was formed in 2002 when the Company hired more than 700 professionals who had been affiliated with the internal audit, business and technology risk consulting practice of Arthur Andersen, including more than 50 individuals who had been partners of that firm. These professionals formed the base of Protiviti.\\nIn 2006, Protiviti acquired the assets of PG Lewis & Associates, a leading national provider of Data Forensics and Cybersecurity services founded in 2003 by serial technology entrepreneur, Paul G. Lewis. Financial terms were not disclosed.\\nThe following year Protiviti acquired the bankruptcy consulting firm PENTA Advisory Services, LLC with locations in Baltimore, Maryland and Richmond, Virginia. PENTA provided restructuring and insolvency services, litigation services and US bankruptcy trustee services. Financial details regarding the transaction were not disclosed.\\nIn January 2019, Protiviti expanded its Middle East & North African presence by launching an office in Cairo, Egypt. The new location is the first Member Firm in North Africa, and is led by Managing Director Ashraf Fahmy, a former Deloitte partner in Egypt and with the firm\\'s enterprise risk practice in Abu Dhabi.\\nIn February 2019, Protiviti added Gauteng-based internal audit and forensic services firm SekelaXabiso CA (SkX) as its first member firm in South Africa. The new firm will serve domestic firms as well as international firms looking for support to enter the South African market. The firm houses over 200 consultants, and has offices in the major financial centres of Gauteng and Durban.\\nFor fiscal year 2019, Protiviti\\'s revenue exceeded US$1 billion for the first time in its 18-year history.\\nIn March 2020, Protiviti expanded its European footprint by opening 3 new offices in ZÃ¼rich, Switzerland, and Berlin and DÃ¼sseldorf, Germany. Protiviti\\'s global network of member firms operate as independently owned and operated entities, but have access to the firm\\'s resources despite not possessing agency to act on Protiviti\\'s behalf.\\n\\n\\n== Service Expansion ==\\nIn March 2022, Protiviti formed a new service line, \"Protiviti Digital\", which serves as a digital marketing agency. Protiviti Digital serves clients seeking to execute complex digital and marketing strategies, as well as transform customer experiences.\\nIn April 2022, Robert Half moved its legal consulting service line to Protiviti, which allows Protiviti to expand its legal consulting practice. The service line supports clients with a broader range of legal, compliance, governance, technology, investigation and transaction-related business needs.\\n\\n\\n== See also ==\\nList of management consulting firms\\nList of IT consulting firms\\nRobert Half\\nArthur Andersen\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Fractal Analytics', 'summary': 'Fractal Analytics Private Limited, trading as Fractal, is a multinational artificial intelligence company which provides services in packaged consumer goods, insurance, healthcare, life sciences, retail, technology, and the financial sector. The company has dual headquarters in Mumbai and New York City, with a presence in the United States, India, and the United Kingdom, among other locations.', 'source': 'https://en.wikipedia.org/wiki/Fractal_Analytics'}, page_content='Fractal Analytics Private Limited, trading as Fractal, is a multinational artificial intelligence company which provides services in packaged consumer goods, insurance, healthcare, life sciences, retail, technology, and the financial sector. The company has dual headquarters in Mumbai and New York City, with a presence in the United States, India, and the United Kingdom, among other locations.\\n\\n\\n== History ==\\nFractal Analytics was founded in 2000 in Mumbai by Srikanth Velamakanni, Pranay Agrawal, Nirmal Palaparthi, Pradeep Suryanarayan and Ramakrishna Reddy. It later moved to the US in 2005. In 2015 they acquired Imagna Analytics and Mobius Innovations.\\nIn 2016, Fractal Analytics appointed Pranay Agrawal as the CEO to replace co-founder Srikanth Velamakanni, the new Group Chief Executive and Executive Vice-Chairman. It also expanded its operations including the creation of two new subsidiaries Qure.ai and Cuddle.ai.\\nIn August 2016, they partnered with KNIME, an open source data analytics platform. In June 2017, they acquired Chicago-based strategy & analytics firm, 4i Inc. In September 2017, they partnered with Final Mile to combine data science with behavioral science  In March 2018, Fractal Analytics acquired behavioural architecture company Final Mile. In January 2019, Fractal received a $200 million funding from Apax Partners.\\nIn January 2022, Fractal became a unicorn company after raising $360 million from private equity firm TPG.\\n\\n\\n== References ==')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akilesh\\OneDrive\\Desktop\\GenAI_Projects\\GenAI_venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x22317c76ef0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore=Chroma.from_documents(docs,embedding=embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#ChatMessageHistory: Keeps track and stored the chat messages exchanged between user and AI model\n",
    "#BasechatHistory: It is like a Template/Abstract that defines the structure of history management. ChatMessageHistory builds on top of this template\n",
    "#RunnableWithMessageHistory: This Wraps chat like a chatbot and save chat history for future conversations making sure past conversations influence the responses.\n",
    "\n",
    "store={}\n",
    "\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Akilesh and I am a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant specialing in Data Analysis. Answer all questions to the best of your ability in. There is a meeting of the Data Analysis team of Company Northeastern and there are several discussions help. The meeting Transcripts are recorded and understand the {transcript} and answer the Questions of the employee\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Akilesh,  \\n\\nGood news! Your work on the ETL process and churn analysis is important to the team. We're fixing data consistency issues, so make sure to thoroughly test your changes before going live. Double-check data flow, transformations, and accuracy.\\n\\nFor churn analysis, keep refining the machine learning models. Gopi can help you prioritize important features using correlation tests and SHAP values.\\n\\nWe need a preliminary user engagement dashboard by Friday and a churn analysis summary by next week. Keep up the great work!\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee=\"Akilesh\"\n",
    "designation=\"Junior Data Analyst\"\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=f\"\"\"Can you summarize the meeting for {employee} who is {designation}.\n",
    "                               Based on the designation refine the most important points that refine 80% essence of the conversation refined to the \n",
    "                               designation and what they will need as takeaways from the meeting. So keeping these factors generate a summary in  in 100 words\n",
    "                               \"\"\")],\"transcript\":transcript_content},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot: Continue chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great question, Akilesh!  You want to be extra sure your ETL changes are rock solid before they go live. Here's a checklist to guide your testing:\\n\\n**Functionality:**\\n\\n* **Data Flow:**  Make sure data moves smoothly from source to destination. Test each step of your pipeline.\\n* **Transformations:** Verify your data transformations are accurate. Compare outputs to expected results.\\n* **Data Quality:**\\n\\n    * **Completeness:**  All required fields have data.\\n    * **Accuracy:**  Values are correct and error-free.\\n    * **Consistency:**  Data follows format and standard rules.\\n\\n**Performance:**\\n\\n* **Load Testing:** Simulate high data volumes to see how your pipeline handles peak loads.\\n* **Latency Testing:** Measure how long it takes data to move through each pipeline stage.\\n\\n**Regression:**\\n\\n* **Previous Functionality:**  Test that your changes didn't break anything that was working before.\\n* **Data Consistency:**  Re-run tests on historical data to ensure your changes didn't introduce inconsistencies.\\n\\n**Other Important Tests:**\\n\\n* **Error Handling:**  How does your pipeline handle errors? Make sure it recovers gracefully or alerts properly.\\n* **Security:**  Did you follow security best practices?\\n\\n\\n**Testing Environment:**\\n\\n* **Mirror Production:** Use a staging environment that closely resembles your production setup.\\n\\nRemember, thorough testing in a controlled environment saves headaches down the line!\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What all tests I should do before before pushing them to production\")], \"transcript\":transcript_content},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load documents from Wikipedia\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load documents related to \"Data Analytics in Finance\" from Wikipedia\n",
    "docs = WikipediaLoader(query=\"Data Analytics in Finance\", load_max_docs=5).load()\n",
    "\n",
    "# Step 2: Initialize the embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 3: Store the documents in a Chroma vector store using the embeddings\n",
    "vectorstore = Chroma.from_documents(docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load the meeting transcript\n",
    "with open(\"transcript.txt\", \"r\") as file:\n",
    "    transcript_content = file.read()\n",
    "\n",
    "# Step 5: Define the prompt template for the initial summary generation\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant specializing in Data Analysis. Answer all questions to the best of your ability. There is a meeting of the Data Analysis team at Company Northeastern. The meeting transcripts are recorded. Understand the {transcript} and answer the questions of the employee.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 6: Combine prompt with the model for generating the summary\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set up session history using ChatMessageHistory and RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Store the session history in a dictionary\n",
    "store = {}\n",
    "\n",
    "# Define a function to get the session history\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap the chain with the history management\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Akilesh,\n",
      "\n",
      "Here's a summary of today's meeting focusing on points relevant to you as a Junior Data Analyst:\n",
      "\n",
      "**Churn Analysis:**\n",
      "\n",
      "* **Great progress!** Your work on the churn prediction model is progressing well, reaching 85% accuracy. \n",
      "* **Feature importance:**  Focus on refining features for the model. Test and prioritize features using correlation tests and SHAP values to identify the most impactful ones for churn prediction.\n",
      "* **Executive summary:** Prepare a detailed executive summary of your findings for the end of next week, highlighting key insights and potential business impact of the model on customer retention.\n",
      "\n",
      "**User Engagement Dashboard:**\n",
      "\n",
      "* **De-duplication:** Your work on removing redundant data in the ETL process is crucial for improving the dashboard's performance. Keep up the great work on that!\n",
      "* **Data integrity:** The team is addressing data inconsistencies across sources, especially with timestamps. Ensure data integrity is maintained for accurate metrics.\n",
      "\n",
      "**General:**\n",
      "\n",
      "* **Teamwork:**  The team is collaborative and supportive. \n",
      "* **Testing:**  Remember to thoroughly test your code before pushing to production, covering unit tests, integration tests, performance tests, and data quality tests.\n",
      "\n",
      "**Action Items:**\n",
      "* **Collaborate with Gopi:**\n",
      "\n",
      "Work with Gopi to refine the churn model's features using correlation tests and SHAP values.\n",
      "* **Complete Executive Summary:** Finalize the churn analysis executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Define the initial user prompt\n",
    "employee = \"Akilesh\"\n",
    "designation = \"Junior Data Analyst\"\n",
    "\n",
    "# employee = \"Elon\"\n",
    "# designation = \"Vice President of BI Team\"\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "\n",
    "# Step 9: Call the model to generate the initial summary based on the transcript\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        'messages': [\n",
    "            HumanMessage(content=f\"\"\"Can you summarize the meeting for {employee}, who is {designation}? \n",
    "                                     Based on the designation, refine the most important points and what \n",
    "                                     they will need as takeaways from the meeting. Generate a summary in 100 words.\"\"\"\n",
    "            )\n",
    "        ],\n",
    "        'transcript': transcript_content  # Passing the meeting transcript as context\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the generated summary\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Riya,\n",
      "\n",
      "Here's a summary of today's meeting, focusing on points relevant to your role as a Junior Data Analyst:\n",
      "\n",
      "**Churn Analysis:**\n",
      "\n",
      "* **Great progress!**  You and Akilesh are doing a great job on the churn prediction model, achieving 85% accuracy.\n",
      "* **Data Consistency is Key:**  Ensure data integrity across Salesforce and user engagement data is consistent, especially with timestamps. This is critical for accurate churn analysis.\n",
      "\n",
      "**User Engagement Dashboard:**\n",
      "* **Data Deduplication:** The team is working on removing redundant data in the ETL process, which is crucial for the dashboard's performance.\n",
      "\n",
      "**General:**\n",
      "\n",
      "* **Teamwork:** Everyone is working together to solve challenges and deliver on projects.\n",
      "\n",
      "**Action Items:**\n",
      "* **Data Integrity:** Make sure timestamps are consistent across Salesforce and user engagement data.\n",
      "* **Executive Summary:** Help Akilesh on the executive summary due next week. \n",
      "* **Data Quality:**  Remember to test thoroughly before pushing code to production, focusing on data quality and accuracy.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration:** Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:**  Before pushing to production, make sure to thoroughly test your work.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:** \n",
      "\n",
      "**Action Item:**  Help Akilesh finalize the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:**  Always remember to test thoroughly before pushing code to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Action Items:**\n",
      "* **Data Consistency:**  Ensure data consistency across Salesforce and user engagement data.\n",
      "\n",
      "\n",
      "\n",
      "* **Executive Summary:** Help Akilesh finalize the executive summary by the end of next week.\n",
      "\n",
      "* **Testing:**  Always remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration:** Continue working closely with Akilesh on the churn model and data integrity.\n",
      "* **Testing:** Always remember to thoroughly test your code before pushing it to production.\n",
      "* **Action Items:**\n",
      "\n",
      "* **Data Consistency:  Ensure data consistency across Salesforce and user engagement data.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **Executive Summary:  Help Akilesh finalize the executive summary by the end of next week. \n",
      "* **Testing:**  Always remember to thoroughly test your code before pushing it to production. \n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing: Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "* **Collaboration:  Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration:  Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing: Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration:  Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before pushing it to production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Remember to thoroughly test your code before production.\n",
      "\n",
      "\n",
      "\n",
      "* **Timeline:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Collaboration: Continue working closely with Akilesh on the churn model and data integrity.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "* **Collaboration:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "* **Collaboration:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **The executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "*\n",
      "\n",
      "\n",
      "\n",
      "* **Testing:  Ensure timely completion of the executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **The executive summary by the end of next week.\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "**\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "*\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "**\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "**\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "*\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "\n",
      "* **The\n",
      "\n",
      "\n",
      "* **The\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Define the initial user prompt\n",
    "employee = \"Riya\"\n",
    "designation = \"Junior Data Analyst\"\n",
    "\n",
    "# employee = \"Elon\"\n",
    "# designation = \"Vice President of BI Team\"\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "\n",
    "# Step 9: Call the model to generate the initial summary based on the transcript\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        'messages': [\n",
    "            HumanMessage(content=f\"\"\"Can you summarize the meeting for {employee}, who is {designation}? \n",
    "                                     Based on the designation, refine the most important points and what \n",
    "                                     they will need as takeaways from the meeting. Generate a summary in 100 words.\"\"\"\n",
    "            )\n",
    "        ],\n",
    "        'transcript': transcript_content  # Passing the meeting transcript as context\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the generated summary\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Define a function to retrieve relevant documents from the vector store\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create a retriever to fetch relevant documents\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Define a function to retrieve documents and combine them into context\n",
    "def retrieve_documents(query: str):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    # Combine the content of the retrieved documents into a single string to pass as context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    return context\n",
    "\n",
    "# Create a Runnable that wraps this retriever\n",
    "retriever_runnable = RunnableLambda(retrieve_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot for Akilesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akilesh\\AppData\\Local\\Temp\\ipykernel_28640\\887328800.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a great question! Before pushing any data analysis model or system to production, you should conduct a thorough battery of tests to ensure accuracy, reliability, and robustness. Here's a breakdown of essential tests:\n",
      "\n",
      "**1. Unit Tests:**\n",
      "\n",
      "* **Individual Component Testing:**  Test each piece of your code (functions, modules) in isolation to ensure they perform as expected. \n",
      "* **Data Validation:** Verify that your code correctly handles different data types, formats, and edge cases (e.g., missing values, outliers).\n",
      "\n",
      "**2. Integration Tests:**\n",
      "\n",
      "* **System-Level Testing:** Test how different parts of your system work together. For example, if you have data processing, modeling, and visualization components, test their interactions.\n",
      "\n",
      "**3. Performance Tests:**\n",
      "\n",
      "* **Load Testing:**  Simulate a high volume of user requests or data processing to ensure your system can handle peak loads without crashing or degrading performance.\n",
      "* **Stress Testing:** Push your system beyond its normal limits to identify breaking points and potential bottlenecks.\n",
      "\n",
      "**4. Regression Tests:**\n",
      "\n",
      "* **Maintain Existing Functionality:**  After making changes, run regression tests to ensure that existing features still work correctly.\n",
      "\n",
      "**5. Data Quality Tests:**\n",
      "\n",
      "* **Accuracy:**  Verify that your model outputs are accurate and consistent with expected results.\n",
      "* **Completeness:**  Ensure that all necessary data is being processed and used by the model.\n",
      "* **Timeliness:**  Test that data is being updated and used in a timely manner.\n",
      "\n",
      "**6. Security Tests:**\n",
      "\n",
      "* **Data Protection:**  Test your system's ability to protect sensitive data from unauthorized access or breaches.\n",
      "* **Authentication:** Ensure that users are properly authenticated before accessing data or functionalities.\n",
      "\n",
      "**7. User Acceptance Testing (UAT):**\n",
      "\n",
      "* **Real-World Feedback:** Have end-users test your system in a real-world scenario to ensure it meets their needs and expectations.\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "* **Test Data:** Use representative test data that covers a wide range of scenarios.\n",
      "* **Test Environment:** Create a dedicated test environment that closely mimics your production environment.\n",
      "* **Continuous Testing:** Integrate testing into your development workflow to catch issues early.\n",
      "* **Documentation:**  Thoroughly document your testing process and results.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Set up the new RAG chain with document retrieval and model generation\n",
    "rag_chain = {\n",
    "    \"transcript\": retriever_runnable,  # Retrieve relevant documents as 'transcript'\n",
    "    \"messages\": RunnablePassthrough()  # Pass through the user's messages\n",
    "} | prompt | model\n",
    "\n",
    "# Step 12: Future query using document retrieval from vector store\n",
    "user_query = \"What all tests should I do before pushing them to production?\"\n",
    "\n",
    "# Retrieve relevant documents and pass them along with the user's query\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"transcript\": retrieve_documents(user_query)  # Fetch the relevant documents from the vector store\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot for Elon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are key metrics you can discuss with management about the progress of your data analysis projects, categorized for clarity:\n",
      "\n",
      "**1. Churn Analysis Project:**\n",
      "\n",
      "* **Model Accuracy:** Report the current accuracy of the churn prediction model. Highlight any improvements made since last reporting.\n",
      "* **Feature Importance:** Share insights on the most influential features driving churn predictions. This helps understand customer segments at highest risk.\n",
      "* **Business Impact:** Estimate the potential impact of the model on churn rate reduction and revenue retention. Quantify this impact whenever possible (e.g., \"Model predicts X% reduction in churn, saving Y dollars annually\").\n",
      "* **Deployment Timeline:**  Provide a realistic timeline for deploying the model into production, including key milestones.\n",
      "\n",
      "**2. User Engagement Dashboard:**\n",
      "\n",
      "* **Data Coverage:** Report the percentage of data successfully integrated into the dashboard. Specify any remaining data sources to be incorporated.\n",
      "* **Real-Time Update Frequency:** Indicate the frequency of real-time data updates on the dashboard (e.g., every 5 minutes, hourly).\n",
      "* **Key Performance Indicators (KPIs):**  Highlight the most important metrics tracked on the dashboard (e.g., active users, session duration, conversion rates). \n",
      "* **User Feedback:** Share any feedback received from stakeholders or users on the dashboard's design, usability, or usefulness.\n",
      "\n",
      "**3. Data Anomaly Alerts:**\n",
      "\n",
      "* **Alert Accuracy:** Report the percentage of true positive alerts vs. false positives.\n",
      "* **Time to Resolution:**  Track the average time it takes to investigate and resolve alerts.\n",
      "* **System Performance:**  Monitor the system's performance in generating alerts and handling data volume.\n",
      "* **Actionable Insights:**  Showcase how the alerts have led to identifying and addressing data issues.\n",
      "\n",
      "**4. General Project Management:**\n",
      "\n",
      "* **Budget vs. Actual:**  Track project spending against the allocated budget.\n",
      "* **Schedule Adherence:** Report on the project's progress against the planned timeline.\n",
      "* **Risk Assessment:** Identify and discuss any potential risks or challenges that may impact project deliverables.\n",
      "\n",
      "\n",
      "**Presentation Tips:**\n",
      "\n",
      "* **Use Visualizations:** Charts, graphs, and dashboards effectively communicate progress and trends.\n",
      "* **Focus on Business Impact:**  Connect project metrics to tangible business outcomes.\n",
      "* **Be Transparent:**  Openly discuss challenges and mitigation plans.\n",
      "* **Propose Solutions:**  Don't just highlight problems; offer actionable solutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Set up the new RAG chain with document retrieval and model generation\n",
    "rag_chain = {\n",
    "    \"transcript\": retriever_runnable,  # Retrieve relevant documents as 'transcript'\n",
    "    \"messages\": RunnablePassthrough()  # Pass through the user's messages\n",
    "} | prompt | model\n",
    "\n",
    "# Step 12: Future query using document retrieval from vector store\n",
    "user_query = \"What all are the key metrics on the projects progress that I can discuss with management\"\n",
    "\n",
    "# Retrieve relevant documents and pass them along with the user's query\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"transcript\": retrieve_documents(user_query)  # Fetch the relevant documents from the vector store\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
