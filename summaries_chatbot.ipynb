{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000256F91A66E0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000256F91D00A0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcript.txt\",\"r\") as file:\n",
    "    transcript_content=file.read() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document: Internal Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Analytics', 'summary': 'Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data, which also falls under and directly relates to the umbrella term, data science. Analytics also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.\\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to International Data Corporation, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.  \\n\\n', 'source': 'https://en.wikipedia.org/wiki/Analytics'}, page_content='Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data, which also falls under and directly relates to the umbrella term, data science. Analytics also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.\\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to International Data Corporation, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.  \\n\\n\\n== Analytics vs analysis ==\\nData analysis focuses on the process of examining past data through business understanding, data understanding, data preparation, modeling and evaluation, and deployment. It is a subset of data analytics, which takes multiple data analysis processes to focus on why an event happened and what may happen in the future based on the previous data. Data analytics is used to formulate larger organizational decisions. \\nData analytics is a multidisciplinary field. There is extensive use of computer skills, mathematics, statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data through analytics. There is increasing use of the term advanced analytics, typically used to describe the technical aspects of analytics, especially in the emerging fields such as the use of machine learning techniques like neural networks, decision trees, logistic regression, linear to multiple regression analysis, and classification to do predictive modeling. It also includes unsupervised machine learning techniques like cluster analysis, principal component analysis, segmentation profile analysis and association analysis.\\n\\n\\n== Applications ==\\n\\n\\n=== Marketing optimization ===\\nMarketing organizations use analytics to determine the outcomes of campaigns or efforts, and to guide decisions for investment and consumer targeting. Demographic studies, customer segmentation, conjoint analysis and other techniques allow marketers to use large amounts of consumer purchase, survey and panel data to understand and communicate marketing strategy.\\nMarketing analytics consists of both qualitative and quantitative, structured and unstructured data used to drive strategic decisions about brand and revenue outcomes. The process involves predictive modelling, marketing experimentation, automation and real-time sales communications. The data enables companies to make predictions and alter strategic execution to maximize performance results.\\nWeb analytics allows marketers to collect session-level information about interactions on a website using an operation called sessionization. Google Analytics is an example of a popular free analytics tool that marketers use for this purpose. Those interactions provide web analytics information systems with the information necessary to track the referrer, search keywords, identify the IP address, and track the activities of the visitor. With this information, a marketer can improve marketing campaigns, website creative content, and information architecture.\\nAnalysis techniques frequently used in marketing inc'), Document(metadata={'title': 'Big data', 'summary': 'Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.\\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17Ã—260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than â‚¬100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Big_data'}, page_content='Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.\\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17Ã—260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than â‚¬100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or '), Document(metadata={'title': 'Predictive analytics', 'summary': 'Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.\\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Predictive_analytics'}, page_content='Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.\\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\\n\\n\\n== Definition ==\\nPredictive analytics involves using statistical techniques and machine learning algorithms to analyze historical data and make forecasts about future events. The risks include data privacy issues, potential biases in data leading to inaccurate predictions, and over - reliance on automated systems. Extending the Value of Your Data Warehousing Investment. |url=http://download.101com.com/pub/tdwi/files/pa_report_q107_f.pdf}}</ref> Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs. The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.\\nPredictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analyticsâ€”Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\" In future industrial systems, the value of predictive analytics will be to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization.\\n\\n\\n== Analytical techniques ==\\nThe approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.\\n\\n\\n=== Machine Learning ===\\n\\nMachine learning can be defined as the ability of a machine to learn and then mimic human behavior that requires intelligence. This is accomplished through artificial intelligence, algorithms, and models.\\n\\n\\n=== Autoregressive Integrated Moving Average (ARIMA) ===\\n\\nARIMA models are a common example of time series models. These models use autoregression, which means the model can be fitted with a regression software that will use machine learning to do most of the regression analysis and smoothing. ARIMA models are known to have no overall trend, but instead have a variation around the average that has a constant amplitude, resulting in stati'), Document(metadata={'title': 'Fractal Analytics', 'summary': 'Fractal Analytics Private Limited, trading as Fractal, is a multinational artificial intelligence company which provides services in packaged consumer goods, insurance, healthcare, life sciences, retail, technology, and the financial sector. The company has dual headquarters in Mumbai and New York City, with a presence in the United States, India, and the United Kingdom, among other locations.', 'source': 'https://en.wikipedia.org/wiki/Fractal_Analytics'}, page_content='Fractal Analytics Private Limited, trading as Fractal, is a multinational artificial intelligence company which provides services in packaged consumer goods, insurance, healthcare, life sciences, retail, technology, and the financial sector. The company has dual headquarters in Mumbai and New York City, with a presence in the United States, India, and the United Kingdom, among other locations.\\n\\n\\n== History ==\\nFractal Analytics was founded in 2000 in Mumbai by Srikanth Velamakanni, Pranay Agrawal, Nirmal Palaparthi, Pradeep Suryanarayan and Ramakrishna Reddy. It later moved to the US in 2005. In 2015 they acquired Imagna Analytics and Mobius Innovations.\\nIn 2016, Fractal Analytics appointed Pranay Agrawal as the CEO to replace co-founder Srikanth Velamakanni, the new Group Chief Executive and Executive Vice-Chairman. It also expanded its operations including the creation of two new subsidiaries Qure.ai and Cuddle.ai.\\nIn August 2016, they partnered with KNIME, an open source data analytics platform. In June 2017, they acquired Chicago-based strategy & analytics firm, 4i Inc. In September 2017, they partnered with Final Mile to combine data science with behavioral science  In March 2018, Fractal Analytics acquired behavioural architecture company Final Mile. In January 2019, Fractal received a $200 million funding from Apax Partners.\\nIn January 2022, Fractal became a unicorn company after raising $360 million from private equity firm TPG.\\n\\n\\n== References =='), Document(metadata={'title': 'Protiviti', 'summary': \"Protiviti Inc. (Protiviti) is a global consulting firm headquartered in Menlo Park and San Ramon, California, that provides consulting in internal audit, risk and compliance, technology, business processes, data analytics and finance. It is a subsidiary under Robert Half. Protiviti and its independently and locally owned Member Firms serve clients through a network of more than 85 locations in over 27 countries.\\nProtiviti has served more than 80 percent of Fortune 100, nearly 80 percent of Fortune 500 and 70 percent of Fortune 1000 companies. From 2018 to 2022, the firm has been consistently listed by Forbes\\u2063 as being one of the world's best management consulting firms. Protiviti has also been listed as one of the 100 Best Companies to Work For by Fortune Magazine for 8 consecutive years from 2015 to 2022.\", 'source': 'https://en.wikipedia.org/wiki/Protiviti'}, page_content='Protiviti Inc. (Protiviti) is a global consulting firm headquartered in Menlo Park and San Ramon, California, that provides consulting in internal audit, risk and compliance, technology, business processes, data analytics and finance. It is a subsidiary under Robert Half. Protiviti and its independently and locally owned Member Firms serve clients through a network of more than 85 locations in over 27 countries.\\nProtiviti has served more than 80 percent of Fortune 100, nearly 80 percent of Fortune 500 and 70 percent of Fortune 1000 companies. From 2018 to 2022, the firm has been consistently listed by Forbes\\u2063 as being one of the world\\'s best management consulting firms. Protiviti has also been listed as one of the 100 Best Companies to Work For by Fortune Magazine for 8 consecutive years from 2015 to 2022.\\n\\n\\n== History ==\\nProtiviti was formed in 2002 when the Company hired more than 700 professionals who had been affiliated with the internal audit, business and technology risk consulting practice of Arthur Andersen, including more than 50 individuals who had been partners of that firm. These professionals formed the base of Protiviti.\\nIn 2006, Protiviti acquired the assets of PG Lewis & Associates, a leading national provider of Data Forensics and Cybersecurity services founded in 2003 by serial technology entrepreneur, Paul G. Lewis. Financial terms were not disclosed.\\nThe following year Protiviti acquired the bankruptcy consulting firm PENTA Advisory Services, LLC with locations in Baltimore, Maryland and Richmond, Virginia. PENTA provided restructuring and insolvency services, litigation services and US bankruptcy trustee services. Financial details regarding the transaction were not disclosed.\\nIn January 2019, Protiviti expanded its Middle East & North African presence by launching an office in Cairo, Egypt. The new location is the first Member Firm in North Africa, and is led by Managing Director Ashraf Fahmy, a former Deloitte partner in Egypt and with the firm\\'s enterprise risk practice in Abu Dhabi.\\nIn February 2019, Protiviti added Gauteng-based internal audit and forensic services firm SekelaXabiso CA (SkX) as its first member firm in South Africa. The new firm will serve domestic firms as well as international firms looking for support to enter the South African market. The firm houses over 200 consultants, and has offices in the major financial centres of Gauteng and Durban.\\nFor fiscal year 2019, Protiviti\\'s revenue exceeded US$1 billion for the first time in its 18-year history.\\nIn March 2020, Protiviti expanded its European footprint by opening 3 new offices in ZÃ¼rich, Switzerland, and Berlin and DÃ¼sseldorf, Germany. Protiviti\\'s global network of member firms operate as independently owned and operated entities, but have access to the firm\\'s resources despite not possessing agency to act on Protiviti\\'s behalf.\\n\\n\\n== Service Expansion ==\\nIn March 2022, Protiviti formed a new service line, \"Protiviti Digital\", which serves as a digital marketing agency. Protiviti Digital serves clients seeking to execute complex digital and marketing strategies, as well as transform customer experiences.\\nIn April 2022, Robert Half moved its legal consulting service line to Protiviti, which allows Protiviti to expand its legal consulting practice. The service line supports clients with a broader range of legal, compliance, governance, technology, investigation and transaction-related business needs.\\n\\n\\n== See also ==\\nList of management consulting firms\\nList of IT consulting firms\\nRobert Half\\nArthur Andersen\\n\\n\\n== References ==')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"Data Analytics in Finance\", load_max_docs=5).load()\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Analytics', 'summary': 'Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data, which also falls under and directly relates to the umbrella term, data science. Analytics also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.\\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to International Data Corporation, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.  \\n\\n', 'source': 'https://en.wikipedia.org/wiki/Analytics'}, page_content='Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data, which also falls under and directly relates to the umbrella term, data science. Analytics also entails applying data patterns toward effective decision-making. It can be valuable in areas rich with recorded information; analytics relies on the simultaneous application of statistics, computer programming, and operations research to quantify performance.\\nOrganizations may apply analytics to business data to describe, predict, and improve business performance. Specifically, areas within analytics include descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics, and cognitive analytics. Analytics may apply to a variety of fields such as marketing, management, finance, online systems, information security, and software services. Since analytics can require extensive computation (see big data), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics. According to International Data Corporation, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. As per Gartner, the overall analytic platforms software market grew by $25.5 billion in 2020.  \\n\\n\\n== Analytics vs analysis ==\\nData analysis focuses on the process of examining past data through business understanding, data understanding, data preparation, modeling and evaluation, and deployment. It is a subset of data analytics, which takes multiple data analysis processes to focus on why an event happened and what may happen in the future based on the previous data. Data analytics is used to formulate larger organizational decisions. \\nData analytics is a multidisciplinary field. There is extensive use of computer skills, mathematics, statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data through analytics. There is increasing use of the term advanced analytics, typically used to describe the technical aspects of analytics, especially in the emerging fields such as the use of machine learning techniques like neural networks, decision trees, logistic regression, linear to multiple regression analysis, and classification to do predictive modeling. It also includes unsupervised machine learning techniques like cluster analysis, principal component analysis, segmentation profile analysis and association analysis.\\n\\n\\n== Applications ==\\n\\n\\n=== Marketing optimization ===\\nMarketing organizations use analytics to determine the outcomes of campaigns or efforts, and to guide decisions for investment and consumer targeting. Demographic studies, customer segmentation, conjoint analysis and other techniques allow marketers to use large amounts of consumer purchase, survey and panel data to understand and communicate marketing strategy.\\nMarketing analytics consists of both qualitative and quantitative, structured and unstructured data used to drive strategic decisions about brand and revenue outcomes. The process involves predictive modelling, marketing experimentation, automation and real-time sales communications. The data enables companies to make predictions and alter strategic execution to maximize performance results.\\nWeb analytics allows marketers to collect session-level information about interactions on a website using an operation called sessionization. Google Analytics is an example of a popular free analytics tool that marketers use for this purpose. Those interactions provide web analytics information systems with the information necessary to track the referrer, search keywords, identify the IP address, and track the activities of the visitor. With this information, a marketer can improve marketing campaigns, website creative content, and information architecture.\\nAnalysis techniques frequently used in marketing inc'),\n",
       " Document(metadata={'title': 'Big data', 'summary': 'Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.\\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17Ã—260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than â‚¬100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\". What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Big_data'}, page_content='Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Though used sometimes loosely partly due to a lack of formal definition, the best interpretation is that it is a large body of information that cannot be comprehended when used in small amounts only.\\nBig data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization\\'s capacity to create and capture value from big data.\\nCurrent usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that\\'s not the most relevant characteristic of this new data ecosystem.\"\\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.\\nThe size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world\\'s technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.17Ã—260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021. While Statista report, the global big data market is forecasted to grow to $103 billion by 2027. In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. In the developed economies of Europe, government administrators could save more than â‚¬100 billion ($149 billion) in operational efficiency improvements alone by using big data. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.\\nRelational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or '),\n",
       " Document(metadata={'title': 'Predictive analytics', 'summary': 'Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.\\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Predictive_analytics'}, page_content='Predictive analytics is a form of business analytics applying machine learning to generate a predictive model for certain business applications. As such, it encompasses a variety of statistical techniques from predictive modeling and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events. It represents a major subset of machine learning applications; in some contexts, it is synonymous with machine learning.\\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.\\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\\n\\n\\n== Definition ==\\nPredictive analytics involves using statistical techniques and machine learning algorithms to analyze historical data and make forecasts about future events. The risks include data privacy issues, potential biases in data leading to inaccurate predictions, and over - reliance on automated systems. Extending the Value of Your Data Warehousing Investment. |url=http://download.101com.com/pub/tdwi/files/pa_report_q107_f.pdf}}</ref> Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs. The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.\\nPredictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analyticsâ€”Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\" In future industrial systems, the value of predictive analytics will be to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization.\\n\\n\\n== Analytical techniques ==\\nThe approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.\\n\\n\\n=== Machine Learning ===\\n\\nMachine learning can be defined as the ability of a machine to learn and then mimic human behavior that requires intelligence. This is accomplished through artificial intelligence, algorithms, and models.\\n\\n\\n=== Autoregressive Integrated Moving Average (ARIMA) ===\\n\\nARIMA models are a common example of time series models. These models use autoregression, which means the model can be fitted with a regression software that will use machine learning to do most of the regression analysis and smoothing. ARIMA models are known to have no overall trend, but instead have a variation around the average that has a constant amplitude, resulting in stati'),\n",
       " Document(metadata={'title': 'Fractal Analytics', 'summary': 'Fractal Analytics Private Limited, trading as Fractal, is a multinational artificial intelligence company which provides services in packaged consumer goods, insurance, healthcare, life sciences, retail, technology, and the financial sector. The company has dual headquarters in Mumbai and New York City, with a presence in the United States, India, and the United Kingdom, among other locations.', 'source': 'https://en.wikipedia.org/wiki/Fractal_Analytics'}, page_content='Fractal Analytics Private Limited, trading as Fractal, is a multinational artificial intelligence company which provides services in packaged consumer goods, insurance, healthcare, life sciences, retail, technology, and the financial sector. The company has dual headquarters in Mumbai and New York City, with a presence in the United States, India, and the United Kingdom, among other locations.\\n\\n\\n== History ==\\nFractal Analytics was founded in 2000 in Mumbai by Srikanth Velamakanni, Pranay Agrawal, Nirmal Palaparthi, Pradeep Suryanarayan and Ramakrishna Reddy. It later moved to the US in 2005. In 2015 they acquired Imagna Analytics and Mobius Innovations.\\nIn 2016, Fractal Analytics appointed Pranay Agrawal as the CEO to replace co-founder Srikanth Velamakanni, the new Group Chief Executive and Executive Vice-Chairman. It also expanded its operations including the creation of two new subsidiaries Qure.ai and Cuddle.ai.\\nIn August 2016, they partnered with KNIME, an open source data analytics platform. In June 2017, they acquired Chicago-based strategy & analytics firm, 4i Inc. In September 2017, they partnered with Final Mile to combine data science with behavioral science  In March 2018, Fractal Analytics acquired behavioural architecture company Final Mile. In January 2019, Fractal received a $200 million funding from Apax Partners.\\nIn January 2022, Fractal became a unicorn company after raising $360 million from private equity firm TPG.\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Protiviti', 'summary': \"Protiviti Inc. (Protiviti) is a global consulting firm headquartered in Menlo Park and San Ramon, California, that provides consulting in internal audit, risk and compliance, technology, business processes, data analytics and finance. It is a subsidiary under Robert Half. Protiviti and its independently and locally owned Member Firms serve clients through a network of more than 85 locations in over 27 countries.\\nProtiviti has served more than 80 percent of Fortune 100, nearly 80 percent of Fortune 500 and 70 percent of Fortune 1000 companies. From 2018 to 2022, the firm has been consistently listed by Forbes\\u2063 as being one of the world's best management consulting firms. Protiviti has also been listed as one of the 100 Best Companies to Work For by Fortune Magazine for 8 consecutive years from 2015 to 2022.\", 'source': 'https://en.wikipedia.org/wiki/Protiviti'}, page_content='Protiviti Inc. (Protiviti) is a global consulting firm headquartered in Menlo Park and San Ramon, California, that provides consulting in internal audit, risk and compliance, technology, business processes, data analytics and finance. It is a subsidiary under Robert Half. Protiviti and its independently and locally owned Member Firms serve clients through a network of more than 85 locations in over 27 countries.\\nProtiviti has served more than 80 percent of Fortune 100, nearly 80 percent of Fortune 500 and 70 percent of Fortune 1000 companies. From 2018 to 2022, the firm has been consistently listed by Forbes\\u2063 as being one of the world\\'s best management consulting firms. Protiviti has also been listed as one of the 100 Best Companies to Work For by Fortune Magazine for 8 consecutive years from 2015 to 2022.\\n\\n\\n== History ==\\nProtiviti was formed in 2002 when the Company hired more than 700 professionals who had been affiliated with the internal audit, business and technology risk consulting practice of Arthur Andersen, including more than 50 individuals who had been partners of that firm. These professionals formed the base of Protiviti.\\nIn 2006, Protiviti acquired the assets of PG Lewis & Associates, a leading national provider of Data Forensics and Cybersecurity services founded in 2003 by serial technology entrepreneur, Paul G. Lewis. Financial terms were not disclosed.\\nThe following year Protiviti acquired the bankruptcy consulting firm PENTA Advisory Services, LLC with locations in Baltimore, Maryland and Richmond, Virginia. PENTA provided restructuring and insolvency services, litigation services and US bankruptcy trustee services. Financial details regarding the transaction were not disclosed.\\nIn January 2019, Protiviti expanded its Middle East & North African presence by launching an office in Cairo, Egypt. The new location is the first Member Firm in North Africa, and is led by Managing Director Ashraf Fahmy, a former Deloitte partner in Egypt and with the firm\\'s enterprise risk practice in Abu Dhabi.\\nIn February 2019, Protiviti added Gauteng-based internal audit and forensic services firm SekelaXabiso CA (SkX) as its first member firm in South Africa. The new firm will serve domestic firms as well as international firms looking for support to enter the South African market. The firm houses over 200 consultants, and has offices in the major financial centres of Gauteng and Durban.\\nFor fiscal year 2019, Protiviti\\'s revenue exceeded US$1 billion for the first time in its 18-year history.\\nIn March 2020, Protiviti expanded its European footprint by opening 3 new offices in ZÃ¼rich, Switzerland, and Berlin and DÃ¼sseldorf, Germany. Protiviti\\'s global network of member firms operate as independently owned and operated entities, but have access to the firm\\'s resources despite not possessing agency to act on Protiviti\\'s behalf.\\n\\n\\n== Service Expansion ==\\nIn March 2022, Protiviti formed a new service line, \"Protiviti Digital\", which serves as a digital marketing agency. Protiviti Digital serves clients seeking to execute complex digital and marketing strategies, as well as transform customer experiences.\\nIn April 2022, Robert Half moved its legal consulting service line to Protiviti, which allows Protiviti to expand its legal consulting practice. The service line supports clients with a broader range of legal, compliance, governance, technology, investigation and transaction-related business needs.\\n\\n\\n== See also ==\\nList of management consulting firms\\nList of IT consulting firms\\nRobert Half\\nArthur Andersen\\n\\n\\n== References ==')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akilesh\\OneDrive\\Desktop\\GenAI_Projects\\GenAI_venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\Akilesh\\OneDrive\\Desktop\\GenAI_Projects\\GenAI_venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Akilesh\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x256f91d0af0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore=Chroma.from_documents(docs,embedding=embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "#ChatMessageHistory: Keeps track and stored the chat messages exchanged between user and AI model\n",
    "#BasechatHistory: It is like a Template/Abstract that defines the structure of history management. ChatMessageHistory builds on top of this template\n",
    "#RunnableWithMessageHistory: This Wraps chat like a chatbot and save chat history for future conversations making sure past conversations influence the responses.\n",
    "\n",
    "store={}\n",
    "\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Akilesh and I am a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Akilesh! It's a pleasure to meet you.\\n\\nBeing a Chief AI Engineer is a fascinating role. What kind of projects are you working on these days? \\n\\nI'm eager to learn more about your work in the field of AI.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant specialing in Data Analysis. Answer all questions to the best of your ability in. There is a meeting of the Data Analysis team of Company Northeastern and there are several discussions help. The meeting Transcripts are recorded and understand the {transcript} and answer the Questions of the employee\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Akilesh, \\n\\nHere's a quick summary of today's meeting focusing on your role:\\n\\n- **User Engagement Dashboard Delay:**  The team is working on a real-time dashboard, but data aggregation issues are causing a delay. Your focus should be on de-duplicating datasets and ensuring data integrity between Salesforce and app data.  \\n\\n- **Client Churn Analysis:** Your work on the churn prediction model is progressing well (85% accuracy!).  Focus on refining features by testing their correlation with churn and using techniques like SHAP values to prioritize the most impactful ones.  \\n\\n- **Data Alerts System:**  We're tweaking the real-time alert system for data anomalies. Be prepared to help refine thresholds and filters to reduce false positives. \\n\\n\\nLet me know if you have any questions! \\n\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee=\"Akilesh\"\n",
    "designation=\"Junior Data Analyst\"\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "repsonse=with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content=f\"\"\"Can you summarize the meeting for {employee} who is {designation}.\n",
    "                               Based on the designation refine the most important points that refine 80% essence of the conversation refined to the \n",
    "                               designation and what they will need as takeaways from the meeting. So keeping these factors generate a summary in  in 100 words\n",
    "                               \"\"\")],\"transcript\":transcript_content},\n",
    "    config=config\n",
    ")\n",
    "repsonse.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot: Continue chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great question, Akilesh!  Before pushing your data pipeline changes and churn analysis model to production, you should absolutely conduct thorough testing. Here's a breakdown:\\n\\n**Data Pipeline Tests:**\\n\\n* **Unit Tests:** Test individual components (like data extraction, transformation, and loading functions) in isolation. Make sure each step works as expected with different input data scenarios.\\n* **Integration Tests:** Verify how different parts of your pipeline work together. Test the flow of data from source to destination, checking for data consistency and transformations along the way.\\n* **Data Volume Tests:**  Simulate high-volume data loads to ensure your pipeline can handle peak times without performance issues or errors.\\n* **Error Handling Tests:**  Test how your pipeline handles invalid data, connection errors, and other potential issues. Make sure it gracefully recovers and logs errors appropriately.\\n* **Regression Tests:** After making changes, run existing tests to ensure you haven't introduced any unintended side effects or broken functionality.\\n\\n**Churn Prediction Model Tests:**\\n\\n* **Training Data Split:** Ensure your training data is split into appropriate sets (training, validation, and testing) to accurately evaluate model performance.\\n* **Model Accuracy:** Evaluate the model's accuracy on the testing data using metrics like precision, recall, F1-score, and AUC.\\n* **Feature Importance:** Analyze feature importance using techniques like SHAP values to understand which features are most influential in the model's predictions.\\n* **Cross-Validation:** Use cross-validation techniques to assess the model's ability to generalize to unseen data and avoid overfitting.\\n* **Bias and Fairness:**  Test for potential bias in the model's predictions across different customer segments.\\n\\n**Deployment & Monitoring:**\\n\\n* **Production Environment Setup:**  Thoroughly test your deployment process in a staging environment that mirrors your production setup.\\n* **Monitoring & Alerting:** Set up monitoring systems to track model performance, data pipeline health, and alert on any unexpected issues.\\n\\n\\nRemember, thorough testing is crucial for ensuring the reliability, accuracy, and stability of your work in production!\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What all tests I should do before before pushing them to production\")], \"transcript\":transcript_content},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load documents from Wikipedia\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Load documents related to \"Data Analytics in Finance\" from Wikipedia\n",
    "docs = WikipediaLoader(query=\"Data Analytics in Finance\", load_max_docs=5).load()\n",
    "\n",
    "# Step 2: Initialize the embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 3: Store the documents in a Chroma vector store using the embeddings\n",
    "vectorstore = Chroma.from_documents(docs, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load the meeting transcript\n",
    "with open(\"transcript.txt\", \"r\") as file:\n",
    "    transcript_content = file.read()\n",
    "\n",
    "# Step 5: Define the prompt template for the initial summary generation\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant specializing in Data Analysis. Answer all questions to the best of your ability. There is a meeting of the Data Analysis team at Company Northeastern. The meeting transcripts are recorded. Understand the {transcript} and answer the questions of the employee.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 6: Combine prompt with the model for generating the summary\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set up session history using ChatMessageHistory and RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Store the session history in a dictionary\n",
    "store = {}\n",
    "\n",
    "# Define a function to get the session history\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap the chain with the history management\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history, input_messages_key=\"messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Akilesh!  Good news - we're making good progress on the user engagement dashboard and churn analysis.  \n",
      "\n",
      "The main hurdle is data aggregation, taking longer than expected due to increased data volume. We're working on optimizing queries and potentially shifting some processing to Snowflake.  \n",
      "\n",
      "For churn analysis, focus on refining the machine learning models, especially feature selection. Gopi suggested using correlation tests and SHAP values to identify the most impactful features. Keep refining the real-time data anomaly alert system and aim for a polished executive summary of churn findings by next week.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Define the initial user prompt\n",
    "employee = \"Akilesh\"\n",
    "designation = \"Junior Data Analyst\"\n",
    "\n",
    "# employee = \"Elon\"\n",
    "# designation = \"Vice President of BI Team\"\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "\n",
    "# Step 9: Call the model to generate the initial summary based on the transcript\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        'messages': [\n",
    "            HumanMessage(content=f\"\"\"Can you summarize the meeting for {employee}, who is {designation}? \n",
    "                                     Based on the designation, refine the most important points and what \n",
    "                                     they will need as takeaways from the meeting. Generate a summary in 100 words.\"\"\"\n",
    "            )\n",
    "        ],\n",
    "        'transcript': transcript_content  # Passing the meeting transcript as context\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the generated summary\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Riya!  The meeting focused on data issues impacting the user engagement dashboard and churn analysis.  \n",
      "\n",
      "We need to tackle redundant data pulls in our ETL process to optimize performance.  Focus on ensuring consistent timestamps across datasets, especially Salesforce and app engagement data.  For churn analysis, explore the impact of payment history and support ticket data on model accuracy.\n",
      "\n",
      "Gopi suggested using SHAP values to understand feature importance.  We're aiming for a clear executive summary of churn findings by next week, highlighting key insights and actionable recommendations.  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Define the initial user prompt\n",
    "employee = \"Riya\"\n",
    "designation = \"Junior Data Analyst\"\n",
    "\n",
    "# employee = \"Elon\"\n",
    "# designation = \"Vice President of BI Team\"\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "\n",
    "# Step 9: Call the model to generate the initial summary based on the transcript\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        'messages': [\n",
    "            HumanMessage(content=f\"\"\"Can you summarize the meeting for {employee}, who is {designation}? \n",
    "                                     Based on the designation, refine the most important points and what \n",
    "                                     they will need as takeaways from the meeting. Generate a summary in 100 words.\"\"\"\n",
    "            )\n",
    "        ],\n",
    "        'transcript': transcript_content  # Passing the meeting transcript as context\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the generated summary\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Define a function to retrieve relevant documents from the vector store\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create a retriever to fetch relevant documents\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Define a function to retrieve documents and combine them into context\n",
    "def retrieve_documents(query: str):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    # Combine the content of the retrieved documents into a single string to pass as context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    return context\n",
    "\n",
    "# Create a Runnable that wraps this retriever\n",
    "retriever_runnable = RunnableLambda(retrieve_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot for Akilesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akilesh\\AppData\\Local\\Temp\\ipykernel_14508\\887328800.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pushing any code to production, it's crucial to conduct a thorough set of tests to ensure its quality, stability, and reliability. Here's a comprehensive checklist of tests to consider:\n",
      "\n",
      "**1. Unit Tests:**\n",
      "\n",
      "* **Functionality:** Test individual units or functions in isolation to verify they perform as expected.\n",
      "* **Edge Cases:**  Test with boundary values, invalid inputs, and unexpected scenarios to ensure robustness.\n",
      "* **Performance:** Measure the execution time and resource consumption of individual units under different loads.\n",
      "\n",
      "**2. Integration Tests:**\n",
      "\n",
      "* **Inter-Module Communication:** Test how different components of your system interact with each other.\n",
      "* **Data Flow:** Verify that data is correctly passed between modules and systems.\n",
      "* **Third-Party APIs:** Test interactions with external APIs or services.\n",
      "\n",
      "**3. End-to-End (E2E) Tests:**\n",
      "\n",
      "* **Workflows:** Simulate complete user journeys or business processes from start to finish.\n",
      "* **System Behavior:** Test the overall system behavior under various conditions and user interactions.\n",
      "\n",
      "**4. System Tests:**\n",
      "\n",
      "* **Load Testing:**  Evaluate the system's performance under high user loads or traffic.\n",
      "* **Stress Testing:** Push the system beyond its expected limits to identify breaking points.\n",
      "* **Performance Tuning:**  Optimize the system for speed, efficiency, and scalability.\n",
      "\n",
      "**5. Security Tests:**\n",
      "\n",
      "* **Vulnerability Scanning:** Identify potential security weaknesses in the code and infrastructure.\n",
      "* **Penetration Testing:** Simulate real-world attacks to assess the system's resilience to breaches.\n",
      "* **Authentication & Authorization:** Test user login, access control, and data protection mechanisms.\n",
      "\n",
      "**6. Regression Tests:**\n",
      "\n",
      "* **Previous Functionality:** Ensure that new changes haven't introduced regressions or broken existing features.\n",
      "* **Code Reviews:** Conduct code reviews to identify potential issues and ensure code quality.\n",
      "\n",
      "**7. Acceptance Tests:**\n",
      "\n",
      "* **User Acceptance Testing (UAT):** Have actual users test the system to validate that it meets their requirements.\n",
      "* **Business Acceptance Testing (BAT):**  Test against business rules and objectives to ensure alignment.\n",
      "\n",
      "**8. Documentation:**\n",
      "\n",
      "* **User Manuals:** Provide clear instructions for users on how to interact with the system.\n",
      "* **Technical Documentation:** Document code structure, APIs, and system architecture.\n",
      "\n",
      "**Tools and Technologies:**\n",
      "\n",
      "* **Testing Frameworks:** JUnit, pytest, Selenium, Cypress, etc.\n",
      "* **Testing Tools:** Postman, LoadRunner, JMeter, etc.\n",
      "* **Version Control:** Git, GitHub, Bitbucket, etc.\n",
      "* **Continuous Integration/Continuous Deployment (CI/CD):** Jenkins, CircleCI, Travis CI, etc.\n",
      "\n",
      "\n",
      "\n",
      "Remember to:\n",
      "\n",
      "* **Prioritize Tests:** Focus on the most critical functionality and potential risks.\n",
      "* **Automate Tests:** Automate as many tests as possible to save time and ensure consistency.\n",
      "* **Track Test Results:**  Use a test management system to track test cases, results, and defects.\n",
      "* **Iterate and Improve:** Continuously review and refine your testing process based on feedback and experience.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Step 11: Set up the new RAG chain with document retrieval and model generation\n",
    "rag_chain = {\n",
    "    \"transcript\": retriever_runnable,  # Retrieve relevant documents as 'transcript'\n",
    "    \"messages\": RunnablePassthrough()  # Pass through the user's messages\n",
    "} | prompt | model\n",
    "\n",
    "# Step 12: Future query using document retrieval from vector store\n",
    "user_query = \"What all tests should I do before pushing them to production?\"\n",
    "\n",
    "# Retrieve relevant documents and pass them along with the user's query\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"transcript\": retrieve_documents(user_query)  # Fetch the relevant documents from the vector store\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot for Elon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of key project progress metrics you can discuss with management, tailored for different aspects of the project:\n",
      "\n",
      "**1. Completion Status & Timeline:**\n",
      "\n",
      "* **Percentage Complete:**  A straightforward measure of overall progress, expressed as a percentage.\n",
      "* **Tasks Completed:** Count of completed tasks against the total planned tasks.\n",
      "* **Timeline Adherence:**  \n",
      "    *  Are you on schedule? \n",
      "    *  Are there any delays, and what are their causes?\n",
      "* **Burn Rate:** Tracks the rate at which you're spending resources (time, budget) over time. Useful for forecasting future needs.\n",
      "\n",
      "**2. Deliverables & Quality:**\n",
      "\n",
      "* **Deliverables Completed:** List of tangible outputs (reports, code, prototypes, etc.) delivered on time and to spec.\n",
      "* **Defect Rate:** Number of defects found divided by the total number of units tested (e.g., lines of code, features).\n",
      "* **Test Coverage:** Percentage of code or functionality covered by automated tests.\n",
      "\n",
      "**3. Team Performance & Collaboration:**\n",
      "\n",
      "* **Team Velocity:** Measures the amount of work a team can complete in a given time period (e.g., story points per sprint).\n",
      "* **Meeting Efficiency:** Track how effectively meetings are used to progress the project.\n",
      "* **Communication Channels:**  Analyze the effectiveness of communication tools and processes.\n",
      "\n",
      "**4. Risks & Issues:**\n",
      "\n",
      "* **Identified Risks:** List of potential risks and their likelihood and impact.\n",
      "* **Mitigations in Place:**  Actions taken or planned to address identified risks.\n",
      "* **Open Issues:**  List of unresolved problems or roadblocks, their impact, and proposed solutions.\n",
      "\n",
      "**5. Budget & Resource Allocation:**\n",
      "\n",
      "* **Actual vs. Planned Spending:**  Track actual expenditure against the original budget.\n",
      "* **Resource Utilization:** Percentage of time team members are actively working on project tasks.\n",
      "* **Resource Requirements:**  Forecasted need for additional resources (personnel, tools, etc.)\n",
      "\n",
      "**Visualizations:**\n",
      "\n",
      "* **Progress Charts:**  Use bar charts, Gantt charts, or Kanban boards to visually represent progress.\n",
      "* **Dashboards:**  Create interactive dashboards to provide a comprehensive overview of key metrics.\n",
      "\n",
      "**Remember to:**\n",
      "\n",
      "* **Tailor Metrics:** Choose metrics that are most relevant to your project and management's priorities.\n",
      "* **Contextualize Data:** Don't just present numbers; explain their significance and implications for the project.\n",
      "* **Focus on Actionable Insights:**  Identify trends, problems, or opportunities that require attention and propose solutions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Set up the new RAG chain with document retrieval and model generation\n",
    "rag_chain = {\n",
    "    \"transcript\": retriever_runnable,  # Retrieve relevant documents as 'transcript'\n",
    "    \"messages\": RunnablePassthrough()  # Pass through the user's messages\n",
    "} | prompt | model\n",
    "\n",
    "# Step 12: Future query using document retrieval from vector store\n",
    "user_query = \"What all are the key metrics on the projects progress that I can discuss with management\"\n",
    "\n",
    "# Retrieve relevant documents and pass them along with the user's query\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"transcript\": retrieve_documents(user_query)  # Fetch the relevant documents from the vector store\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
